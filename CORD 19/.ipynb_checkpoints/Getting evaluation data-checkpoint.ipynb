{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wget\n",
    "# loading relevancy data\n",
    "\n",
    "path = 'C:/Users/User/OneDrive - NTNU/NTNU/Prosjekt oppgave NLP/dataset/CORD-19/'\n",
    "file = 'relevance_data.csv'\n",
    "\n",
    "relevance_data = pd.read_csv(path + file)\n",
    "relevance_data.head()\n",
    "\n",
    "\n",
    "url_txt = \"https://ir.nist.gov/covidSubmit/data/qrels-covid_d5_j0.5-5.txt\"\n",
    "filname_txt = wget.download(url_txt)\n",
    "\n",
    "#%%\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "topics = {}\n",
    "root = ET.parse(\"topics-rnd5.xml\").getroot()\n",
    "for topic in root.findall(\"topic\"):\n",
    "    topic_number = topic.attrib[\"number\"]\n",
    "    topics[topic_number] = {}\n",
    "    for query in topic.findall(\"query\"):\n",
    "        topics[topic_number][\"query\"] = query.text\n",
    "    for question in topic.findall(\"question\"):\n",
    "        topics[topic_number][\"question\"] = question.text\n",
    "    for narrative in topic.findall(\"narrative\"):\n",
    "        topics[topic_number][\"narrative\"] = narrative.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..........................................................................] 1142244 / 1142244\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labeled_data_cord_19_question = []\n",
    "print(\"\")\n",
    "for i in range(50):\n",
    "    top_id = (i+1)\n",
    "    print(top_id)\n",
    "    dat = {'query_id': str(top_id),\n",
    "          'query': topics[str(top_id)][\"question\"],\n",
    "          'relevant_docs': []}\n",
    "    \n",
    "    rel_docs = []\n",
    "    pd = relevance_data[relevance_data.topic_id == top_id]\n",
    "    \n",
    "    \n",
    "    for index, row in pd.iterrows():\n",
    "        rel_docs.append({\n",
    "            'id': row['cord_uid'],\n",
    "            'score': row['relevancy']\n",
    "        })\n",
    "    \n",
    "    dat['relevant_docs'] = rel_docs\n",
    "    \n",
    "    labeled_data_cord_19_question.append(dat)\n",
    "    \n",
    "labeled_data_cord_19_question\n",
    "\n",
    "import pickle\n",
    "\n",
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\\"\n",
    "\n",
    "with open(path + \"labeled_data_cord_19_question.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(labeled_data_cord_19_question, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "6\n",
      "7\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "14\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "labeled_data_cord_19_exclude = []\n",
    "print(\"\")\n",
    "exclude_list = [4,8,13,16,31,32,49]\n",
    "for i in range(50):\n",
    "    if (i+1) not in exclude_list: \n",
    "        top_id = (i+1)\n",
    "        print(top_id)\n",
    "        dat = {'query_id': str(top_id),\n",
    "              'query': topics[str(top_id)][\"query\"],\n",
    "              'relevant_docs': []}\n",
    "\n",
    "        rel_docs = []\n",
    "        pd = relevance_data[relevance_data.topic_id == top_id]\n",
    "\n",
    "\n",
    "        for index, row in pd.iterrows():\n",
    "            rel_docs.append({\n",
    "                'id': row['cord_uid'],\n",
    "                'score': row['relevancy']\n",
    "            })\n",
    "\n",
    "        dat['relevant_docs'] = rel_docs\n",
    "\n",
    "        labeled_data_cord_19_exclude.append(dat)\n",
    "    \n",
    "labeled_data_cord_19_exclude\n",
    "\n",
    "import pickle\n",
    "\n",
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\\"\n",
    "\n",
    "with open(path + \"labeled_data_cord_19_exclude.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(labeled_data_cord_19_exclude, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "labeled_data_cord_19 = []\n",
    "print(\"\")\n",
    "for i in range(50):\n",
    "    top_id = (i+1)\n",
    "    print(top_id)\n",
    "    dat = {'query_id': str(top_id),\n",
    "          'query': topics[str(top_id)][\"query\"],\n",
    "          'relevant_docs': []}\n",
    "    \n",
    "    rel_docs = []\n",
    "    pd = relevance_data[relevance_data.topic_id == top_id]\n",
    "    \n",
    "    \n",
    "    for index, row in pd.iterrows():\n",
    "        \n",
    "        if row['relevancy'] > 0:\n",
    "            rel_docs.append({\n",
    "                'id': row['cord_uid'],\n",
    "                'score': row['relevancy']\n",
    "            })\n",
    "    \n",
    "    dat['relevant_docs'] = rel_docs\n",
    "    \n",
    "    labeled_data_cord_19.append(dat)\n",
    "    \n",
    "labeled_data_cord_19\n",
    "\n",
    "import pickle\n",
    "\n",
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\\"\n",
    "\n",
    "with open(path + \"labeled_data_cord_19.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(labeled_data_cord_19, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "labeled_data_cord_19_topic_36_50 = []\n",
    "print(\"\")\n",
    "for i in range(35, 50):\n",
    "    top_id = (i+1)\n",
    "    print(top_id)\n",
    "    dat = {'query_id': str(top_id),\n",
    "          'query': topics[str(top_id)][\"query\"],\n",
    "          'relevant_docs': []}\n",
    "    \n",
    "    rel_docs = []\n",
    "    pd = relevance_data[relevance_data.topic_id == top_id]\n",
    "    \n",
    "    \n",
    "    for index, row in pd.iterrows():\n",
    "        \n",
    "        if row['relevancy'] > 0:\n",
    "            rel_docs.append({\n",
    "                'id': row['cord_uid'],\n",
    "                'score': row['relevancy']\n",
    "            })\n",
    "    \n",
    "    dat['relevant_docs'] = rel_docs\n",
    "    \n",
    "    labeled_data_cord_19.append(dat)\n",
    "    \n",
    "labeled_data_cord_19\n",
    "\n",
    "import pickle\n",
    "\n",
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\\"\n",
    "\n",
    "with open(path + \"labeled_data_cord_19_topic_36_50.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(labeled_data_cord_19_topic_36_50, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

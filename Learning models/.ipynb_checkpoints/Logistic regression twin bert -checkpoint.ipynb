{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO add split 1-35 train and 36-50 test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>round_id</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>005b2j4b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>00fmeepz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>010vptx3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0194oljo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>021q9884</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  topic_id  round_id  cord_uid  relevancy\n",
       "0           0         1       4.5  005b2j4b          2\n",
       "1           1         1       4.0  00fmeepz          1\n",
       "2           2         1       0.5  010vptx3          2\n",
       "3           3         1       2.5  0194oljo          1\n",
       "4           4         1       4.0  021q9884          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading relevancy data\n",
    "\n",
    "path = 'C:/Users/User/OneDrive - NTNU/NTNU/Prosjekt oppgave NLP/dataset/CORD-19/'\n",
    "file = 'relevance_data.csv'\n",
    "\n",
    "label_data = pd.read_csv(path + file)\n",
    "label_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>question</th>\n",
       "      <th>narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>what is the origin of COVID-19</td>\n",
       "      <td>seeking range of information about the SARS-Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>coronavirus response to weather changes</td>\n",
       "      <td>how does the coronavirus respond to changes in...</td>\n",
       "      <td>seeking range of information about the SARS-Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>coronavirus immunity</td>\n",
       "      <td>will SARS-CoV2 infected people develop immunit...</td>\n",
       "      <td>seeking studies of immunity developed due to i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>how do people die from the coronavirus</td>\n",
       "      <td>what causes death from Covid-19?</td>\n",
       "      <td>Studies looking at mechanisms of death from Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>animal models of COVID-19</td>\n",
       "      <td>what drugs have been active against SARS-CoV o...</td>\n",
       "      <td>Papers that describe the results  of testing d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id                                    query  \\\n",
       "0         1                       coronavirus origin   \n",
       "1         2  coronavirus response to weather changes   \n",
       "2         3                     coronavirus immunity   \n",
       "3         4   how do people die from the coronavirus   \n",
       "4         5                animal models of COVID-19   \n",
       "\n",
       "                                            question  \\\n",
       "0                     what is the origin of COVID-19   \n",
       "1  how does the coronavirus respond to changes in...   \n",
       "2  will SARS-CoV2 infected people develop immunit...   \n",
       "3                   what causes death from Covid-19?   \n",
       "4  what drugs have been active against SARS-CoV o...   \n",
       "\n",
       "                                           narrative  \n",
       "0  seeking range of information about the SARS-Co...  \n",
       "1  seeking range of information about the SARS-Co...  \n",
       "2  seeking studies of immunity developed due to i...  \n",
       "3  Studies looking at mechanisms of death from Co...  \n",
       "4  Papers that describe the results  of testing d...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading topics data\n",
    "\n",
    "path = 'C:/Users/User/OneDrive - NTNU/NTNU/Prosjekt oppgave NLP/dataset/CORD-19/'\n",
    "file = 'topics.csv'\n",
    "\n",
    "topics = pd.read_csv(path + file)\n",
    "topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69318"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_topics = [i+1 for i in range(35)]\n",
    "test_topics = [i+1 for i in range(35,50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available models: ['bert-base-uncased', \"allenai/scibert_scivocab_uncased\", \"gsarti/covidbert-nli\"]\n",
    "\n",
    "def normalize_tensor(tensor):\n",
    "    tensor_normed = []\n",
    "    norm = torch.norm(tensor)\n",
    "    if norm>0:\n",
    "        tensor_normed = tensor/norm\n",
    "    else:\n",
    "        tensor_normed = tensor\n",
    "    return tensor_normed\n",
    "\n",
    "def get_tesors(row, topic_field, doc_field, model):\n",
    "    topic_path = \"C:/Users/User/Documents/NTNU/NLP/CORD-19/topic_embeddings/\"\n",
    "    doc_path = \"C:/Users/User/Documents/NTNU/NLP/CORD-19/Embeddings/786/\"\n",
    "    doc_id = row[\"cord_uid\"]\n",
    "    topic_id = row[\"topic_id\"]\n",
    "    topic_tensor = []\n",
    "    doc_tensor = []\n",
    "    \n",
    "    doc_embedding_file = doc_path + doc_id + \".txt\"\n",
    "    try:\n",
    "        with open(doc_embedding_file, \"rb\") as fp:   # Unpickling\n",
    "            doc_embedding = pickle.load(fp)\n",
    "    except:\n",
    "        return [[0]], [[0]]\n",
    "        \n",
    "    doc_tensor = doc_embedding[\"models\"][model][doc_field]\n",
    "            \n",
    "    topic_embedding_file = topic_path + str(topic_id) + \".txt\"\n",
    "    with open(topic_embedding_file, \"rb\") as fp:   # Unpickling\n",
    "        topic_embedding = pickle.load(fp)\n",
    "        \n",
    "    topic_tensor =  topic_embedding[\"models\"][model][topic_field]\n",
    "    \n",
    "    topic_tensor = normalize_tensor(topic_tensor)\n",
    "    doc_tensor = normalize_tensor(doc_tensor)\n",
    "    \n",
    "    return topic_tensor, doc_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tensors(tensor_a,tensor_b):\n",
    "    n = len(tensor_a)\n",
    "    m = len(tensor_b)\n",
    "    c = torch.empty(n + m)\n",
    "    c[0:n] = tensor_a\n",
    "    c[n:(n+m)] = tensor_b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(num):\n",
    "    return num != num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(n_datapoints, doc_field, query_field, encoder_model):\n",
    "    \n",
    "    topic_dataset = torch.empty(n_datapoints, 768)\n",
    "    document_dataset = torch.empty(n_datapoints, 768)\n",
    "    rel = torch.empty(n_datapoints)\n",
    "    \n",
    "    miss = 0\n",
    "    i = 0\n",
    "    for idx, row in label_data.iterrows():\n",
    "        if idx < num:\n",
    "            a, b = get_tesors(row, query_field, doc_field, encoder_model)\n",
    "            if len(a[0]) != 1:\n",
    "              \n",
    "                topic_dataset[i] = a[0]\n",
    "                document_dataset[i] = b[0]\n",
    "        \n",
    "                rel[i] = row[\"relevancy\"]\n",
    "                i += 1\n",
    "            else:\n",
    "                miss += 1\n",
    "            if(idx % 5000) == 0:\n",
    "                print(idx)\n",
    "    \n",
    "    document_dataset = document_dataset[0:(num-miss)]\n",
    "    topic_dataset = topic_dataset[0:(num-miss)]\n",
    "    rel = rel[0:(num-miss)]\n",
    "    \n",
    "    return topic_dataset, document_dataset, rel, miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_top_id(type_d, doc_field, query_field, encoder_model):\n",
    "    \n",
    "    label_dat_top = []\n",
    "    if type_d == \"train\":\n",
    "        label_dat_top = label_data[label_data[\"topic_id\"] < 36]\n",
    "    else:\n",
    "        label_dat_top = label_data[label_data[\"topic_id\"] > 35]\n",
    "        \n",
    "    n_datapoints = len(label_dat_top[\"topic_id\"])\n",
    "    print(\"Number of datapoints for \",type_d,\" \",n_datapoints)\n",
    "    topic_dataset = torch.empty(n_datapoints, 768)\n",
    "    document_dataset = torch.empty(n_datapoints, 768)\n",
    "    rel = torch.empty(n_datapoints)\n",
    "    \n",
    "    miss = 0\n",
    "    i = 0\n",
    "    for idx, row in label_dat_top.iterrows():\n",
    "        a, b = get_tesors(row, query_field, doc_field, encoder_model)\n",
    "        if len(a[0]) != 1:\n",
    "\n",
    "            topic_dataset[i] = a[0]\n",
    "            document_dataset[i] = b[0]\n",
    "\n",
    "            rel[i] = row[\"relevancy\"]\n",
    "            i += 1\n",
    "        else:\n",
    "            miss += 1\n",
    "        if(idx % 5000) == 0:\n",
    "            print(idx)\n",
    "    \n",
    "    document_dataset = document_dataset[0:(num-miss)]\n",
    "    topic_dataset = topic_dataset[0:(num-miss)]\n",
    "    rel = rel[0:(num-miss)]\n",
    "    \n",
    "    return topic_dataset, document_dataset, rel, miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_datasets(doc, query, relevancy, num_class, type_join):\n",
    "    relevancy_temp = relevancy\n",
    "    if num_class == 2:\n",
    "        for i, rel in enumerate(relevancy_temp):\n",
    "            if rel > 0.01:\n",
    "                relevancy_temp[i] = 1\n",
    "    n = len(relevancy_temp)\n",
    "    X_dataset = []\n",
    "    \n",
    "    if type_join == \"multi\":\n",
    "        X_dataset = torch.empty(n, 768)\n",
    "        for i in range(n):\n",
    "            X_dataset[i] = doc[i]*query[i]\n",
    "    else: \n",
    "        X_dataset = torch.empty(n, 768*2)\n",
    "        for i in range(n):\n",
    "            X_dataset[i] = join_tensors(doc[i],query[i])\n",
    "    joint_dataset = torch.cat((X_dataset, relevancy_temp.unsqueeze(1)), 1)\n",
    "    return joint_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading \n",
    "num = 40000 # max: 69318\n",
    "\n",
    "available_models = ['bert-base-uncased', \"allenai/scibert_scivocab_uncased\", \"gsarti/covidbert-nli\"]\n",
    "bert_model = available_models[2]\n",
    "query_field = \"query\"\n",
    "doc_field = \"title\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>round_id</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>005b2j4b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>00fmeepz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>010vptx3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0194oljo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>021q9884</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  topic_id  round_id  cord_uid  relevancy\n",
       "0           0         1       4.5  005b2j4b          2\n",
       "1           1         1       4.0  00fmeepz          1\n",
       "2           2         1       0.5  010vptx3          2\n",
       "3           3         1       2.5  0194oljo          1\n",
       "4           4         1       4.0  021q9884          1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dat_top = label_data[label_data[\"topic_id\"] < 35]\n",
    "label_dat_top.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints for  train   52865\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "Number of datapoints for  test   16453\n",
      "55000\n",
      "60000\n",
      "65000\n"
     ]
    }
   ],
   "source": [
    "topic_dat_train, doc_dat_train, rel_dat_train, miss_train  = load_data_top_id(\"train\", doc_field, query_field, bert_model)\n",
    "topic_dat_test, doc_dat_test, rel_dat_test, miss_test  = load_data_top_id(\"test\", doc_field, query_field, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "joining_type = \"multi\"\n",
    "\n",
    "# Model Hyper Parameters \n",
    "input_size = 768*2\n",
    "if joining_type == \"multi\":\n",
    "    input_size = 768\n",
    "\n",
    "model_type = \"annet\"\n",
    "num_classes = 3\n",
    "num_epochs = 20\n",
    "batch_size = 1000\n",
    "learning_rate = 0.003\n",
    "\n",
    "# Proportion of training to test dat\n",
    "training_split = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37937, 768]\n",
      "[37937, 768]\n",
      "[37937]\n",
      "[16453, 768]\n",
      "[16453, 768]\n",
      "[16453]\n"
     ]
    }
   ],
   "source": [
    "print(list(topic_dat_train.size()))\n",
    "print(list(doc_dat_train.size()))\n",
    "print(list(rel_dat_train.size()))\n",
    "print(list(topic_dat_test.size()))\n",
    "print(list(doc_dat_test.size()))\n",
    "print(list(rel_dat_test.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = join_datasets(topic_dat_train, doc_dat_train, rel_dat_train, num_classes, joining_type)\n",
    "test_data = join_datasets(topic_dat_test, doc_dat_test, rel_dat_test, num_classes, joining_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37937, 769]\n",
      "[16453, 769]\n"
     ]
    }
   ],
   "source": [
    "print(list(train_data.size()))\n",
    "print(list(test_data.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6459656799430635\n",
      "0.16058201755542084\n",
      "0.19345230250151568\n",
      "0.5478636114994226\n",
      "0.1651978362608643\n",
      "0.2868169938613019\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "num_dat_points = len(rel_dat)\n",
    "ind = [i for i in range(num_dat_points)]\n",
    "\n",
    "random.shuffle(ind)\n",
    "\n",
    "split_ind = round(0.8*num_dat_points)\n",
    "\n",
    "train_ind = ind[0:split_ind]\n",
    "test_ind = ind[(split_ind+1):num_dat_points]\n",
    "\n",
    "print(list(dat[train_ind].size()))\n",
    "print(list(dat[test_ind].size()))\n",
    "\n",
    "\n",
    "print(rel_dat[train_ind].int().tolist().count(0)/len(train_ind))\n",
    "print(rel_dat[train_ind].int().tolist().count(1)/len(train_ind))\n",
    "if num_classes == 3:\n",
    "    print(rel_dat[train_ind].int().tolist().count(2)/len(train_ind))\n",
    "\"\"\"\n",
    "\n",
    "print(rel_dat_train.int().tolist().count(0)/len(rel_dat_train))\n",
    "print(rel_dat_train.int().tolist().count(1)/len(rel_dat_train))\n",
    "if num_classes == 3:\n",
    "    print(rel_dat_train.int().tolist().count(2)/len(rel_dat_train))\n",
    "\n",
    "print(rel_dat_test.int().tolist().count(0)/len(rel_dat_test))\n",
    "print(rel_dat_test.int().tolist().count(1)/len(rel_dat_test))\n",
    "if num_classes == 3:\n",
    "    print(rel_dat_test.int().tolist().count(2)/len(rel_dat_test))\n",
    "    \n",
    "\n",
    "\n",
    "# Dataset Loader (Input Pipline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 0\n",
    "if model_type == \"simple\":\n",
    "    model = nn.Sequential(\n",
    "                     nn.Linear(input_size, num_classes),\n",
    "                     nn.Sigmoid())\n",
    "elif model_type == \"deep\":\n",
    "     model = nn.Sequential(nn.Linear(input_size, 128),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(128, 64),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(64, num_classes),\n",
    "                         nn.Sigmoid())\n",
    "elif model_type == \"no_relu\":\n",
    "    model = nn.Sequential(nn.Linear(input_size, 768),\n",
    "                         nn.Linear(768, 1))\n",
    "else:\n",
    "    model = nn.Sequential(nn.Linear(input_size, 768),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(768, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid():\n",
    "    \n",
    "\n",
    "    # Test the Model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X_y in test_loader:\n",
    "        y = X_y[:,input_size]\n",
    "        y = y.int()\n",
    "        y =  torch.LongTensor(y.tolist())\n",
    "        #print(y)\n",
    "        X = X_y[:,0:(input_size)].tolist()\n",
    "        X = torch.tensor(X)\n",
    "        outputs = model(X)\n",
    "        #print(outputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum()\n",
    "\n",
    "    print('Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  \\n# Model\\nclass LogisticRegression(nn.Module):\\n    def __init__(self, input_size, num_classes):\\n        super(LogisticRegression, self).__init__()\\n        self.linear = nn.Linear(input_size, num_classes)\\n    \\n    def forward(self, x):\\n        out = self.linear(x)\\n        return out\\nmodel = LogisticRegression(input_size, num_classes)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"  \n",
    "# Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "model = LogisticRegression(input_size, num_classes)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([1000])) that is different to the input size (torch.Size([1000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Loss and Optimizer\n",
    "# Softmax is internally computed.\n",
    "# Set parameters to be updated.\n",
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "loss_list = []\n",
    "valid_list = []\n",
    "# Training the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (X_y) in enumerate(train_loader):\n",
    "        \n",
    "        y = X_y[:,input_size]\n",
    "        #print(y)\n",
    "        X = X_y[:,0:(input_size)].tolist()\n",
    "        #print(X)\n",
    "        #print(list(X.size()))\n",
    "        #print(len(y))\n",
    "        #y = y#.int()\n",
    "        #print(list(y.size()))\n",
    "        #print(len(y))\n",
    "        y =  torch.LongTensor(y.tolist())\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(torch.tensor(X))\n",
    "        loss = criterion(outputs, y)\n",
    "        #print(loss)\n",
    "        #print(model)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss)\n",
    "        if (i) % 400 == 0:\n",
    "            #print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n",
    "            #       % (epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "            print(epoch)\n",
    "            print(loss)\n",
    "    if epoch % 10 == 0:\n",
    "        valid_list.append(valid())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for X_y in test_loader:\n",
    "    y = X_y[:,input_size]\n",
    "    y = y.int()\n",
    "    y =  torch.LongTensor(y.tolist())\n",
    "    #print(y)\n",
    "    X = X_y[:,0:(input_size)].tolist()\n",
    "    X = torch.tensor(X)\n",
    "    outputs = model(X)\n",
    "    print(outputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += y.size(0)\n",
    "    correct += (predicted == y).sum()\n",
    "    \n",
    "print('Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "# Softmax is internally computed.\n",
    "# Set parameters to be updated.\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "\n",
    "# Training the Model\n",
    "for epoch in range(100):\n",
    "    for i, (X_y) in enumerate(train_loader):\n",
    "        \n",
    "        y = X_y[:,input_size]\n",
    "        #print(y)\n",
    "        X = X_y[:,0:(input_size)].tolist()\n",
    "        #print(X)\n",
    "        #print(list(X.size()))\n",
    "        #print(len(y))\n",
    "        y = y.int()\n",
    "        #print(list(y.size()))\n",
    "        #print(len(y))\n",
    "        y =  torch.LongTensor(y.tolist())\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(torch.tensor(X))\n",
    "        loss = criterion(outputs, y)\n",
    "        #print(loss)\n",
    "        #print(model)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss)\n",
    "        if (i) % 400 == 0:\n",
    "            #print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n",
    "            #       % (epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "            print(epoch)\n",
    "            print(loss)\n",
    "    if epoch % 10 == 0:\n",
    "        valid_list.append(valid())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_dat.int().tolist().count(0)/len(rel_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.rand(input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for X_y in train_loader:\n",
    "    y = X_y[:,input_size]\n",
    "    y = y.int()\n",
    "    y =  torch.LongTensor(y.tolist())\n",
    "    #print(y)\n",
    "    X = X_y[:,0:(input_size)].tolist()\n",
    "    X = torch.tensor(X)\n",
    "    outputs = model(X)\n",
    "    #print(outputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += y.size(0)\n",
    "    correct += (predicted == y).sum()\n",
    "    \n",
    "print('Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

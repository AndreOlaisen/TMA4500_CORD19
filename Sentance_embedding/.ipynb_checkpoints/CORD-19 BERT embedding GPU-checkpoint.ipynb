{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37279, 21)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\\"\n",
    "docs = pd.read_csv(path + \"crod_19_only_rel.csv\")\n",
    "docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(model, tokenizer, sentence):\n",
    "    \n",
    "    # Tokens comes from a process that splits the input into sub-entities with interesting linguistic properties.\n",
    "    tokens = tokenizer.tokenize(sentence.lower())\n",
    "\n",
    "    # This is not sufficient for the model, as it requires integers as input, \n",
    "    # not a problem, let's convert tokens to ids.\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    #print(tokens_ids)\n",
    "\n",
    "    # Add the required special tokens\n",
    "    tokens_ids = tokenizer.build_inputs_with_special_tokens(tokens_ids)\n",
    "    #print(tokens_ids)\n",
    "    \n",
    "    # We need to convert to a Deep Learning framework specific format, let's use PyTorch for now.\n",
    "    tokens_pt = torch.tensor([tokens_ids])\n",
    "\n",
    "    # The length of the tokens can not be more than 512\n",
    "    if len(tokens_pt[0]) > 512:\n",
    "        tokens_pt = torch.tensor([tokens_pt[0][0:512].tolist()])\n",
    "    \n",
    "    tokens_pt = tokens_pt.cuda()\n",
    "        \n",
    "    # Now we're ready to go through BERT with out input\n",
    "    outputs, pooled = model(tokens_pt)\n",
    "    return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['bert-base-uncased', \"allenai/scibert_scivocab_uncased\", \"gsarti/covidbert-nli\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = 0\n",
    "bert_model = 0\n",
    "\n",
    "model_id = 0\n",
    "\n",
    "if model_id == 0:\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased').cuda()\n",
    "if model_id == 1:\n",
    "    scibert_tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "    scibert_model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\").cuda()\n",
    "if model_id == 2:\n",
    "    covid_tokenizer = AutoTokenizer.from_pretrained(\"gsarti/covidbert-nli\")\n",
    "    covid_model = AutoModel.from_pretrained(\"gsarti/covidbert-nli\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(num):\n",
    "    return num != num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 2.00 GiB total capacity; 1.18 GiB already allocated; 1.06 MiB free; 10.37 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-99291b5ddc5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0membedd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0membedding\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"models\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfield_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-eda943c6b112>\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(model, tokenizer, sentence)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Now we're ready to go through BERT with out input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpooled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens_pt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpooled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    839\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    480\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m                 )\n\u001b[0;32m    484\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m         )\n\u001b[0;32m    404\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    337\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         )\n\u001b[0;32m    341\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 2.00 GiB total capacity; 1.18 GiB already allocated; 1.06 MiB free; 10.37 MiB cached)"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/User/Documents/NTNU/NLP/CORD-19/Embeddings/786/\"\n",
    "#n_models = 3\n",
    "#n_fields = 2\n",
    "dim = 786\n",
    "\n",
    "start = time.time()\n",
    "m = start\n",
    "\n",
    "field_name = 'abstract'\n",
    "\n",
    "for idx, row in docs.iterrows():\n",
    "    embedding = {}\n",
    "    if (idx < 10):\n",
    "        cord_uid = row[\"cord_uid\"]\n",
    "        embedding = {}\n",
    "        file = path + cord_uid + \".txt\"\n",
    "        with open(file, \"rb\") as fp:   # Unpickling\n",
    "            embedding = pickle.load(fp)\n",
    "\n",
    "        #models = [bert_model, scibert_model, covid_model]\n",
    "        model_name = ['bert-base-uncased', \"allenai/scibert_scivocab_uncased\", \"gsarti/covidbert-nli\"]\n",
    "        #tokenizers = [bert_tokenizer ,scibert_tokenizer, covid_tokenizer]\n",
    "        #fields = [\"title\", \"abstract\"]\n",
    "        \n",
    "        #embedding[\"models\"][model_name[i]] = {}\n",
    "        field = row[field_name]\n",
    "\n",
    "        res = isinstance(field, str) \n",
    "        if (not res) or isNaN(field):\n",
    "            field = row[\"title\"]\n",
    "\n",
    "\n",
    "        embedd = tokenize(bert_model, bert_tokenizer, field)\n",
    "        embedding[\"models\"][model_name[model_id]][field_name] = embedd\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        ## Timing the loops\n",
    "        if (idx % 1000) == 0:\n",
    "            print(idx)\n",
    "            print(\"Time (1000 iterations):\", round(time.time() - m,1))\n",
    "            m = time.time()\n",
    "\n",
    "        with open(file, \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(embedding, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0102, -0.3737, -0.9746,  0.6022,  0.8261, -0.3076,  0.2930,  0.2730,\n",
       "         -0.7471, -0.9276, -0.4815,  0.8626,  0.2386,  0.7950,  0.4367, -0.6525,\n",
       "         -0.1136, -0.3221,  0.0427, -0.2792,  0.5534,  0.9999, -0.7850,  0.4110,\n",
       "          0.1783,  0.9109, -0.2236,  0.4989,  0.5344,  0.1912, -0.2670,  0.3970,\n",
       "         -0.5468, -0.2149, -0.8931, -0.4516,  0.2251,  0.0723, -0.4800, -0.0644,\n",
       "         -0.6586,  0.2833,  0.9620, -0.4194,  0.8487,  0.0230, -0.9996,  0.2169,\n",
       "         -0.3985,  0.7609,  0.9244,  0.8573,  0.3377,  0.5523,  0.4771, -0.6528,\n",
       "          0.1571,  0.2534, -0.4018,  0.0375, -0.2473,  0.4947, -0.8407,  0.0777,\n",
       "          0.8858,  0.8041, -0.4474, -0.5743, -0.1939,  0.1055,  0.4081,  0.0219,\n",
       "         -0.4331, -0.5163,  0.8123,  0.1588,  0.0098,  1.0000, -0.0470, -0.6111,\n",
       "          0.8224,  0.9279,  0.0841, -0.7137,  0.7931, -1.0000,  0.3229, -0.1720,\n",
       "         -0.5004,  0.3240,  0.5632, -0.2917,  0.7976,  0.1278, -0.8568, -0.4486,\n",
       "         -0.2825, -0.8283, -0.3674, -0.7683,  0.3958, -0.3665, -0.5163, -0.1365,\n",
       "          0.4651, -0.3885, -0.2081,  0.5400,  0.8543,  0.3396,  0.3945, -0.3335,\n",
       "          0.4446, -0.1020,  0.3750, -0.4325, -0.4343, -0.1938, -0.5465,  0.2351,\n",
       "         -0.7247, -0.2121,  0.4812, -0.8465,  0.3066, -0.3640, -0.8660, -1.0000,\n",
       "         -0.6836, -0.7822, -0.5683, -0.4152, -0.4427, -0.6080,  0.4335,  0.5291,\n",
       "          0.2641,  0.9530, -0.2932,  0.4907, -0.4643, -0.6413,  0.4366, -0.4550,\n",
       "          0.9339,  0.5436, -0.6481,  0.0371, -0.5039,  0.3820, -0.6130, -0.0892,\n",
       "         -0.8697, -0.7318,  0.0182,  0.3426, -0.4877, -0.9027, -0.7444, -0.3560,\n",
       "         -0.1792,  0.0156,  0.7472,  0.2806, -0.8898,  0.0116,  0.8433,  0.1629,\n",
       "         -0.3907, -0.2846,  0.1652, -0.3823, -0.9392, -0.6217, -0.5109,  0.0056,\n",
       "          0.3021,  0.5830,  0.1082,  0.6902, -0.3818,  0.0449, -0.5224,  0.5254,\n",
       "          0.1142,  0.0986, -0.9002,  0.4630, -0.3991,  0.7212, -0.0993, -0.6145,\n",
       "         -0.3867, -0.5314, -0.1903, -0.3530, -0.9468,  0.6077, -0.1396, -0.1288,\n",
       "         -0.0943,  0.5903,  0.2542,  0.3973,  0.8128,  0.6848, -0.0372, -0.5782,\n",
       "          0.0833,  0.1289,  0.0969,  0.6563, -0.9252, -0.2510, -0.0825, -0.5211,\n",
       "          0.0658, -0.4814, -0.5521, -0.1076,  0.5197, -0.8325,  0.8512,  0.6281,\n",
       "         -0.0973, -0.6755,  0.4013, -0.4930,  0.3256, -0.0095,  0.9576,  0.8871,\n",
       "         -0.2875, -0.1469,  0.8712, -0.8455, -0.5010,  0.2397, -0.4361,  0.2115,\n",
       "         -0.3389,  0.5477,  0.8916,  0.3413,  0.3069, -0.7543,  0.2997, -0.7957,\n",
       "         -0.3918,  0.8532,  0.8615,  0.0968,  0.2812, -0.7686, -0.2823,  0.7272,\n",
       "         -0.9654, -0.5978, -0.9192,  0.0553, -0.6089,  0.7705,  0.2185,  0.9059,\n",
       "         -0.3103, -0.1826, -0.5745,  0.0257,  0.2687,  0.2246, -0.6062, -0.2896,\n",
       "         -0.6549, -0.2474,  0.3335, -0.3937, -0.8082,  0.2024, -0.2589,  0.5442,\n",
       "          0.3747,  0.1953, -0.8731,  0.8903,  1.0000,  0.3937,  0.0442, -0.2234,\n",
       "         -0.9988, -0.8103,  0.9435, -0.9172, -1.0000, -0.0921, -0.3522,  0.3054,\n",
       "         -1.0000, -0.5644, -0.3655, -0.0865,  0.8642,  0.1951,  0.3760, -1.0000,\n",
       "         -0.0364,  0.2194,  0.1615,  0.8581, -0.6341,  0.1510,  0.3970,  0.7846,\n",
       "          0.0113,  0.2607, -0.9589, -0.3747, -0.6191, -0.9237,  0.9954,  0.3565,\n",
       "         -0.6159, -0.3974,  0.6981, -0.2777,  0.7536, -0.5772, -0.4484,  0.6164,\n",
       "          0.5815,  0.5732,  0.0179, -0.2833,  0.1421,  0.8465, -0.0281,  0.0083,\n",
       "          0.0218,  0.4507, -0.6216,  0.2714, -0.7844, -0.6864,  0.5439, -0.3734,\n",
       "          0.7811,  1.0000,  0.7797, -0.4358,  0.6256,  0.0268, -0.7321,  1.0000,\n",
       "          0.4646, -0.5237,  0.1021,  0.5529, -0.5256, -0.4785,  0.8504, -0.1500,\n",
       "         -0.8722, -0.7915,  0.4876, -0.3600,  0.9835, -0.2756, -0.5483,  0.5867,\n",
       "          0.5742, -0.6402, -0.2447,  0.3191, -0.9169,  0.4427,  0.2303,  0.5807,\n",
       "          0.5910, -0.3809,  0.5809, -0.0931, -0.0187,  0.2399, -0.9252, -0.3778,\n",
       "          0.9003,  0.2643, -0.4174, -0.0054, -0.1345, -0.9017, -0.5587,  0.7222,\n",
       "          1.0000, -0.0459,  0.9131, -0.5322, -0.3900,  0.4318,  0.3247,  0.3264,\n",
       "         -0.4353, -0.1787, -0.0172, -0.3859, -0.5903,  0.6233, -0.0579,  0.0392,\n",
       "          0.9955,  0.4523,  0.2412,  0.1738,  0.9489,  0.1715,  0.0987,  0.8855,\n",
       "          0.5744, -0.1826,  0.0329,  0.4837, -0.8897, -0.0583, -0.3673,  0.1789,\n",
       "         -0.7420, -0.4154, -0.2972,  0.6468,  0.8570,  0.3567,  0.3067,  0.6762,\n",
       "          1.0000, -0.8970,  0.2874,  0.5583,  0.4765, -0.9979, -0.0293, -0.3948,\n",
       "         -0.4313, -0.8996, -0.3052,  0.0333, -0.6252,  0.9161,  0.6959, -0.5356,\n",
       "         -0.4282, -0.3923,  0.2451,  0.3426, -0.9753, -0.4227,  0.1795,  0.7982,\n",
       "         -0.3984, -0.7389, -0.6808, -0.3610,  0.3136, -0.5268,  0.0597,  0.9612,\n",
       "         -0.7889, -0.8975, -0.8111, -0.3855, -0.3106,  0.4027, -0.1647, -0.8927,\n",
       "         -0.3264,  1.0000, -0.2815,  0.7951,  0.3072,  0.4480, -0.2155,  0.2532,\n",
       "          0.9405,  0.2873, -0.8843, -0.8855,  0.3180, -0.2925,  0.4583,  0.9245,\n",
       "          0.8833,  0.2632,  0.8786,  0.3487, -0.4328,  0.4947,  0.7840, -0.2654,\n",
       "         -0.1047, -0.2093, -0.4635, -0.5123, -0.2653,  1.0000,  0.1545,  0.7575,\n",
       "         -0.3133, -0.7501, -0.1541,  1.0000,  0.3537, -0.0197,  0.3573, -0.0589,\n",
       "         -0.3053,  0.3832, -0.3440, -0.1022, -0.0393,  0.1624,  0.4014, -0.4699,\n",
       "         -0.7739, -0.5009,  0.1380, -0.6967,  0.9995, -0.4450,  0.0460, -0.2103,\n",
       "         -0.6951, -0.3523,  0.1302, -0.5517, -0.5075,  0.6199,  0.3760,  0.1004,\n",
       "          0.0553, -0.4759,  0.8533,  0.7340, -0.8292, -0.3583,  0.3187, -0.3845,\n",
       "          0.3809,  0.9999,  0.0424,  0.7101,  0.3098, -0.5632,  0.5009, -0.6336,\n",
       "          0.7297, -0.6573, -0.1850, -0.2857,  0.2018, -0.2415, -0.6889, -0.3462,\n",
       "          0.2499, -0.1232, -0.5728, -0.0762,  0.3939,  0.5297, -0.2771, -0.2761,\n",
       "          0.2968, -0.4599, -0.7101, -0.5630, -0.5767, -0.9999,  0.2297, -1.0000,\n",
       "          0.7493,  0.8308, -0.0584,  0.6863,  0.6019,  0.7007, -0.1263, -0.9482,\n",
       "         -0.5674,  0.3083, -0.3314, -0.2782, -0.2289,  0.3476, -0.2875,  0.2817,\n",
       "         -0.7750,  0.3414, -0.2702,  1.0000,  0.2687, -0.5999, -0.7824,  0.2552,\n",
       "         -0.6321,  1.0000, -0.1597, -0.5501,  0.0171, -0.5376, -0.4942,  0.3171,\n",
       "          0.4488, -0.4817, -0.9241,  0.1773,  0.1380, -0.3198,  0.6185,  0.0123,\n",
       "         -0.0796,  0.3404,  0.8328,  0.5206,  0.8387,  0.2947, -0.5513, -0.6237,\n",
       "          0.6431,  0.3034, -0.2564, -0.0670,  1.0000,  0.1497, -0.6728, -0.6392,\n",
       "         -0.7281, -0.2525, -0.2959,  0.4310,  0.2597,  0.3979, -0.1790,  0.3001,\n",
       "         -0.8709,  0.1610, -0.3354, -0.7922,  0.4454, -0.5200, -0.6355, -0.4223,\n",
       "          0.4890, -0.2762, -0.2055,  0.3488,  0.2145,  0.2485,  0.1547, -1.0000,\n",
       "          0.1486,  0.4180,  0.8840,  0.3782,  0.5582,  0.6276,  0.2739, -0.3921,\n",
       "         -0.7805, -0.4191, -0.0763,  0.5445,  0.4860,  0.5592,  0.2380, -0.2293,\n",
       "         -0.7248, -0.8887, -0.7674, -0.5162,  0.2997, -0.7679, -0.2503,  0.6082,\n",
       "          0.2117, -0.0789, -0.7439, -0.8899, -0.5046, -0.0452,  0.0489,  0.1889,\n",
       "          0.5494,  0.0780,  0.0625, -0.0815, -0.8916, -0.0642, -0.3013,  0.5116,\n",
       "          0.8959, -0.3963,  0.4861,  0.6809, -0.5052,  0.2585, -0.5968, -0.0561,\n",
       "          0.7394, -0.2801,  0.7265, -0.2566, -0.2158, -0.0993, -0.0317, -0.1243,\n",
       "         -0.3864,  0.1333,  0.4779,  0.2967,  0.7422, -0.0766, -0.4913, -0.1931,\n",
       "         -0.9061, -0.3069,  0.3956, -0.2560, -0.8523,  0.8497,  0.4506,  0.8581,\n",
       "          0.8985, -0.2320, -0.1010, -0.4227,  0.1797, -0.8420, -0.5944, -0.1203,\n",
       "         -0.0139,  0.2606,  0.9999, -0.8031, -0.9223, -0.7436, -0.2229,  0.3742,\n",
       "         -0.3489, -1.0000, -0.0469, -0.8209,  0.8785, -0.5263,  0.9039, -0.6830,\n",
       "         -0.6747, -0.3914,  0.7212,  0.5195, -0.3267, -0.5312,  0.0545, -0.3881,\n",
       "          0.7920,  0.5042, -0.5536, -0.7275,  0.0631, -0.9637, -0.3257,  0.5181]],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"Until now we have talked about term matching methods for information retrieval. While these methods are good they do have some issues. One issue is when the user is unfamiliar with what terms are used in relevant documents. A user might search for what percentage of people infected by covid die?, while the more 'correct' way might be What is the Covid case fatality rate. These two sentences have more or less the same semantic meaning, but can lead to different results when using term matching. What if we could try to extract the semantic meaning of a sentence, and match it with the query? For this task we will introduce sentence embedding models.\"\n",
    "string2 = \"Before looking more into sentence embedding models we will look at a simpler task; word embedding. The point of this task is to map a word into a vector. This vector should represents the meaning of the word. When the meaning of a word is represented as a vector it can be easily interpreted by a computer. One can understand the main idea of word embedding by a simple equation \"\n",
    "string3 = \"During the global covid-19 pandemic it has been important for researchers, politicians and others to be able to access the newest and most relevant research relating to COVID-19. The Semantic Scholar team at the Allen Institute for AI has partnered with leading research groups and released a dataset called CORD-19 \\cite{Wang2020CORD19TC}. The dataset aims to keep a corpus of COVID-19 research articles so researchers can apply recent advances in natural language processing to aid in the global effort against the pandemic. The CORD-19 dataset contains research articles relating to COVID-19. This includes articles on similar viruses like SARS and MERS, pandemics and other relevant topics. The dataset is updated daily. \"\n",
    "tokenize(covid_model, covid_tokenizer, string+string2+string3+string+string2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scibert_tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "scibert_model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_tokenizer = AutoTokenizer.from_pretrained(\"gsarti/covidbert-nli\")\n",
    "covid_model = AutoModel.from_pretrained(\"gsarti/covidbert-nli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>...</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>mag_id</th>\n",
       "      <th>who_covidence_id</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>pdf_json_files</th>\n",
       "      <th>pmc_json_files</th>\n",
       "      <th>url</th>\n",
       "      <th>s2_id</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2b73a28n</td>\n",
       "      <td>348055649b6b8cf2b9a376498df9bf41f7123605</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Role of endothelin-1 in lung disease</td>\n",
       "      <td>10.1186/rr44</td>\n",
       "      <td>PMC59574</td>\n",
       "      <td>11686871.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Endothelin-1 (ET-1) is a 21 amino acid peptide...</td>\n",
       "      <td>...</td>\n",
       "      <td>Fagan, Karen A; McMurtry, Ivan F; Rodman, David M</td>\n",
       "      <td>Respir Res</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/348055649b6b8cf2b9a37...</td>\n",
       "      <td>document_parses/pmc_json/PMC59574.xml.json</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>from xenopus laevis [16] . eta receptors in no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>zjufx4fo</td>\n",
       "      <td>b2897e1277f56641193a6db73825f707eed3e4c9</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Sequence requirements for RNA strand transfer ...</td>\n",
       "      <td>10.1093/emboj/20.24.7220</td>\n",
       "      <td>PMC125340</td>\n",
       "      <td>11742998.0</td>\n",
       "      <td>green-oa</td>\n",
       "      <td>Nidovirus subgenomic mRNAs contain a leader se...</td>\n",
       "      <td>...</td>\n",
       "      <td>Pasternak, Alexander O.; van den Born, Erwin; ...</td>\n",
       "      <td>The EMBO Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/b2897e1277f56641193a6...</td>\n",
       "      <td>document_parses/pmc_json/PMC125340.xml.json</td>\n",
       "      <td>http://europepmc.org/articles/pmc125340?pdf=re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the genetic information of rna viruses is orga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8zchiykl</td>\n",
       "      <td>5806726a24dc91de3954001effbdffd7a82d54e2</td>\n",
       "      <td>PMC</td>\n",
       "      <td>The 21st International Symposium on Intensive ...</td>\n",
       "      <td>10.1186/cc1013</td>\n",
       "      <td>PMC137274</td>\n",
       "      <td>11353930.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>The 21st International Symposium on Intensive ...</td>\n",
       "      <td>...</td>\n",
       "      <td>Ball, Jonathan; Venn, Richard</td>\n",
       "      <td>Crit Care</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/5806726a24dc91de39540...</td>\n",
       "      <td>document_parses/pmc_json/PMC137274.xml.json</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this year's symposium was dominated by the res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>5tkvsudh</td>\n",
       "      <td>9d4e3e8eb092d5ed282d0aa4aadcaa8b7165b5e9</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Conservation of polyamine regulation by transl...</td>\n",
       "      <td>10.1093/emboj/19.8.1907</td>\n",
       "      <td>PMC302018</td>\n",
       "      <td>10775274.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Regulation of ornithine decarboxylase in verte...</td>\n",
       "      <td>...</td>\n",
       "      <td>Ivanov, Ivaylo P.; Matsufuji, Senya; Murakami,...</td>\n",
       "      <td>EMBO J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/9d4e3e8eb092d5ed282d0...</td>\n",
       "      <td>document_parses/pmc_json/PMC302018.xml.json</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the ef®ciency of +1 ribosomal frameshifting at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>tvxpckxo</td>\n",
       "      <td>d09b79026117ec9faebba46a8d13aa9b23ec751e</td>\n",
       "      <td>PMC</td>\n",
       "      <td>A Method to Identify p62's UBA Domain Interact...</td>\n",
       "      <td>10.1251/bpo66</td>\n",
       "      <td>PMC302190</td>\n",
       "      <td>14702098.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>The UBA domain is a conserved sequence motif a...</td>\n",
       "      <td>...</td>\n",
       "      <td>Pridgeon, Julia W.; Geetha, Thangiah; Wooten, ...</td>\n",
       "      <td>Biol Proced Online</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>document_parses/pdf_json/d09b79026117ec9faebba...</td>\n",
       "      <td>document_parses/pmc_json/PMC302190.xml.json</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>p62 is a novel cellular protein which was init...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  cord_uid                                       sha source_x  \\\n",
       "0           3  2b73a28n  348055649b6b8cf2b9a376498df9bf41f7123605      PMC   \n",
       "1           5  zjufx4fo  b2897e1277f56641193a6db73825f707eed3e4c9      PMC   \n",
       "2           7  8zchiykl  5806726a24dc91de3954001effbdffd7a82d54e2      PMC   \n",
       "3          10  5tkvsudh  9d4e3e8eb092d5ed282d0aa4aadcaa8b7165b5e9      PMC   \n",
       "4          12  tvxpckxo  d09b79026117ec9faebba46a8d13aa9b23ec751e      PMC   \n",
       "\n",
       "                                               title  \\\n",
       "0               Role of endothelin-1 in lung disease   \n",
       "1  Sequence requirements for RNA strand transfer ...   \n",
       "2  The 21st International Symposium on Intensive ...   \n",
       "3  Conservation of polyamine regulation by transl...   \n",
       "4  A Method to Identify p62's UBA Domain Interact...   \n",
       "\n",
       "                        doi      pmcid   pubmed_id   license  \\\n",
       "0              10.1186/rr44   PMC59574  11686871.0     no-cc   \n",
       "1  10.1093/emboj/20.24.7220  PMC125340  11742998.0  green-oa   \n",
       "2            10.1186/cc1013  PMC137274  11353930.0     no-cc   \n",
       "3   10.1093/emboj/19.8.1907  PMC302018  10775274.0     no-cc   \n",
       "4             10.1251/bpo66  PMC302190  14702098.0     no-cc   \n",
       "\n",
       "                                            abstract  ...  \\\n",
       "0  Endothelin-1 (ET-1) is a 21 amino acid peptide...  ...   \n",
       "1  Nidovirus subgenomic mRNAs contain a leader se...  ...   \n",
       "2  The 21st International Symposium on Intensive ...  ...   \n",
       "3  Regulation of ornithine decarboxylase in verte...  ...   \n",
       "4  The UBA domain is a conserved sequence motif a...  ...   \n",
       "\n",
       "                                             authors             journal  \\\n",
       "0  Fagan, Karen A; McMurtry, Ivan F; Rodman, David M          Respir Res   \n",
       "1  Pasternak, Alexander O.; van den Born, Erwin; ...    The EMBO Journal   \n",
       "2                      Ball, Jonathan; Venn, Richard           Crit Care   \n",
       "3  Ivanov, Ivaylo P.; Matsufuji, Senya; Murakami,...              EMBO J   \n",
       "4  Pridgeon, Julia W.; Geetha, Thangiah; Wooten, ...  Biol Proced Online   \n",
       "\n",
       "  mag_id  who_covidence_id arxiv_id  \\\n",
       "0    NaN               NaN      NaN   \n",
       "1    NaN               NaN      NaN   \n",
       "2    NaN               NaN      NaN   \n",
       "3    NaN               NaN      NaN   \n",
       "4    NaN               NaN      NaN   \n",
       "\n",
       "                                      pdf_json_files  \\\n",
       "0  document_parses/pdf_json/348055649b6b8cf2b9a37...   \n",
       "1  document_parses/pdf_json/b2897e1277f56641193a6...   \n",
       "2  document_parses/pdf_json/5806726a24dc91de39540...   \n",
       "3  document_parses/pdf_json/9d4e3e8eb092d5ed282d0...   \n",
       "4  document_parses/pdf_json/d09b79026117ec9faebba...   \n",
       "\n",
       "                                pmc_json_files  \\\n",
       "0   document_parses/pmc_json/PMC59574.xml.json   \n",
       "1  document_parses/pmc_json/PMC125340.xml.json   \n",
       "2  document_parses/pmc_json/PMC137274.xml.json   \n",
       "3  document_parses/pmc_json/PMC302018.xml.json   \n",
       "4  document_parses/pmc_json/PMC302190.xml.json   \n",
       "\n",
       "                                                 url s2_id  \\\n",
       "0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...   NaN   \n",
       "1  http://europepmc.org/articles/pmc125340?pdf=re...   NaN   \n",
       "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...   NaN   \n",
       "3  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...   NaN   \n",
       "4  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...   NaN   \n",
       "\n",
       "                                           body_text  \n",
       "0  from xenopus laevis [16] . eta receptors in no...  \n",
       "1  the genetic information of rna viruses is orga...  \n",
       "2  this year's symposium was dominated by the res...  \n",
       "3  the ef®ciency of +1 ribosomal frameshifting at...  \n",
       "4  p62 is a novel cellular protein which was init...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\\"\n",
    "docs = pd.read_csv(path + \"crod_19_only_rel.csv\")\n",
    "docs.shape\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert embeddings\n",
    "\n",
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/User/Documents/NTNU/NLP/CORD-19/Embeddings/BERT/786/title/\"\n",
    "for idx, row in docs.iterrows():\n",
    "    BERT_embeddings_title = {}\n",
    "    sha = row[\"sha\"]\n",
    "    cord_uid = row[\"cord_uid\"]\n",
    "    BERT_embeddings_title[row[\"sha\"]] = tokenize(bert_model, bert_tokenizer, row[\"title\"])\n",
    "    if (idx % 10000) == 0:\n",
    "        print(idx)\n",
    "    file = path + cord_uid + \".txt\"\n",
    "    with open(file, \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(BERT_embeddings_title, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\embeddings\\\\\"\n",
    "\n",
    "with open(path + \"BERT_embedding_title.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(BERT_embeddings_title, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_embeddings_abstract = {}\n",
    "for idx, row in docs.iterrows():\n",
    "    BERT_embeddings_abstract[row[\"sha\"]] = tokenize(bert_model, bert_tokenizer, row[\"abstract\"])\n",
    "    if (idx % 10000) == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\embeddings\\\\\"\n",
    "\n",
    "with open(path + \"BERT_embedding_abstract.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(BERT_embeddings_abstract, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciBert embeddings\n",
    "\n",
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SciBERT_embeddings_title = {}\n",
    "for idx, row in docs.iterrows():\n",
    "    SciBERT_embeddings_title[row[\"sha\"]] = tokenize(scibert_model, scibert_tokenizer, row[\"title\"])\n",
    "    if (idx % 10000) == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\embeddings\\\\\"\n",
    "\n",
    "with open(path + \"SciBERT_embedding_title.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(SciBERT_embeddings_title, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SciBERT_embeddings_abstract = {}\n",
    "for idx, row in docs.iterrows():\n",
    "    SciBERT_embeddings_abstract[row[\"sha\"]] = tokenize(scibert_model, scibert_tokenizer, row[\"abstract\"])\n",
    "    if (idx % 10000) == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\embeddings\\\\\"\n",
    "\n",
    "with open(path + \"SciBERT_embedding_abstract.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(SciBERT_embeddings_abstract, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CordBert embeddings\n",
    "\n",
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CordBERT_embeddings_title = {}\n",
    "for idx, row in docs.iterrows():\n",
    "    CordBERT_embeddings_title[row[\"sha\"]] = tokenize(covid_model, covid_tokenizer, row[\"title\"])\n",
    "    if (idx % 10000) == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\embeddings\\\\\"\n",
    "\n",
    "with open(path + \"CordBERT_embedding_title.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(CordBERT_embeddings_title, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CordBERT_embeddings_abstract = {}\n",
    "for idx, row in docs.iterrows():\n",
    "    CordBERT_embeddings_abstract[row[\"sha\"]] = tokenize(covid_model, covid_tokenizer, row[\"abstract\"])\n",
    "    if (idx % 10000) == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\embeddings\\\\\"\n",
    "\n",
    "with open(path + \"CordBERT_embedding_abstract.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(CordBERT_embeddings_abstract, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding dim=786"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(num):\n",
    "    return num != num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22000\n",
      "Time (1000 iterations): 2.9\n",
      "23000\n",
      "Time (1000 iterations): 2901.4\n",
      "24000\n",
      "Time (1000 iterations): 3052.3\n",
      "25000\n",
      "Time (1000 iterations): 3092.0\n",
      "26000\n",
      "Time (1000 iterations): 3273.8\n",
      "27000\n",
      "Time (1000 iterations): 3098.5\n",
      "28000\n",
      "Time (1000 iterations): 3359.0\n",
      "29000\n",
      "Time (1000 iterations): 3308.5\n",
      "30000\n",
      "Time (1000 iterations): 2962.8\n",
      "31000\n",
      "Time (1000 iterations): 3120.6\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/User/Documents/NTNU/NLP/CORD-19/Embeddings/786/\"\n",
    "n_models = 3\n",
    "n_fields = 2\n",
    "dim = 786\n",
    "\n",
    "start = time.time()\n",
    "m = start\n",
    "\n",
    "for idx, row in docs.iterrows():\n",
    "    embedding = {}\n",
    "    if (idx>21999):\n",
    "        sha = row[\"sha\"]\n",
    "        cord_uid = row[\"cord_uid\"]\n",
    "        embedding[\"sha\"] = sha\n",
    "        embedding[\"cord_uid\"] = cord_uid\n",
    "        embedding[\"dim\"] = dim\n",
    "        embedding[\"models\"] = {}\n",
    "\n",
    "        models = [bert_model, scibert_model, covid_model]\n",
    "        model_name = ['bert-base-uncased', \"allenai/scibert_scivocab_uncased\", \"gsarti/covidbert-nli\"]\n",
    "        tokenizers = [bert_tokenizer ,scibert_tokenizer, covid_tokenizer]\n",
    "        fields = [\"title\", \"abstract\"]\n",
    "\n",
    "        for i in range(n_models):\n",
    "            embedding[\"models\"][model_name[i]] = {}\n",
    "            for j in range(n_fields):\n",
    "                field = row[fields[j]]\n",
    "\n",
    "                res = isinstance(fields[j], str) \n",
    "                if (not res) or isNaN(field):\n",
    "                    field = row[\"title\"]\n",
    "              \n",
    "                \n",
    "                embedd = tokenize(models[i], tokenizers[i], field)\n",
    "                embedding[\"models\"][model_name[i]][fields[j]] = embedd\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        ## Timing the loops\n",
    "        if (idx % 1000) == 0:\n",
    "            print(idx)\n",
    "            print(\"Time (1000 iterations):\", round(time.time() - m,1))\n",
    "            m = time.time()\n",
    "\n",
    "        file = path + cord_uid + \".txt\"\n",
    "        with open(file, \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(embedding, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{\n",
    "sha: sha,\n",
    "cord_uid: cord_uid,\n",
    "dim: dim\n",
    "models: {model_name1: {title: [...],\n",
    "                       abstract: [...]},\n",
    "         model_name2: {title: [...],\n",
    "                       abstract: [...]},\n",
    "        ...                   \n",
    "}\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

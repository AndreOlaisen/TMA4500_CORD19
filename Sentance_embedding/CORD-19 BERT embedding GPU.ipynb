{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embeddings on GPU\n",
    "This is usually a bit faster, but it seams my gpu has to little memory for the model and the tokens at the same time.\n",
    "So this does not really work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37279, 21)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\\"\n",
    "docs = pd.read_csv(path + \"crod_19_only_rel.csv\")\n",
    "docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(model, tokenizer, sentence):\n",
    "    \n",
    "    # Tokens comes from a process that splits the input into sub-entities with interesting linguistic properties.\n",
    "    tokens = tokenizer.tokenize(sentence.lower())\n",
    "\n",
    "    # This is not sufficient for the model, as it requires integers as input, \n",
    "    # not a problem, let's convert tokens to ids.\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    #print(tokens_ids)\n",
    "\n",
    "    # Add the required special tokens\n",
    "    tokens_ids = tokenizer.build_inputs_with_special_tokens(tokens_ids)\n",
    "    #print(tokens_ids)\n",
    "    \n",
    "    # We need to convert to a Deep Learning framework specific format, let's use PyTorch for now.\n",
    "    tokens_pt = torch.tensor([tokens_ids])\n",
    "\n",
    "    # The length of the tokens can not be more than 512\n",
    "    if len(tokens_pt[0]) > 512:\n",
    "        tokens_pt = torch.tensor([tokens_pt[0][0:512].tolist()])\n",
    "    \n",
    "    tokens_pt = tokens_pt.cuda()\n",
    "        \n",
    "    # Now we're ready to go through BERT with out input\n",
    "    outputs, pooled = model(tokens_pt)\n",
    "    return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['bert-base-uncased', \"allenai/scibert_scivocab_uncased\", \"gsarti/covidbert-nli\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = 0\n",
    "bert_model = 0\n",
    "\n",
    "model_id = 0\n",
    "\n",
    "if model_id == 0:\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased').cuda()\n",
    "if model_id == 1:\n",
    "    scibert_tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "    scibert_model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\").cuda()\n",
    "if model_id == 2:\n",
    "    covid_tokenizer = AutoTokenizer.from_pretrained(\"gsarti/covidbert-nli\")\n",
    "    covid_model = AutoModel.from_pretrained(\"gsarti/covidbert-nli\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(num):\n",
    "    return num != num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/User/Documents/NTNU/NLP/CORD-19/Embeddings/786/\"\n",
    "#n_models = 3\n",
    "#n_fields = 2\n",
    "dim = 786\n",
    "\n",
    "start = time.time()\n",
    "m = start\n",
    "\n",
    "field_name = 'abstract'\n",
    "\n",
    "for idx, row in docs.iterrows():\n",
    "    embedding = {}\n",
    "    if (idx < 10):\n",
    "        torch.cuda.empty_cache()\n",
    "        cord_uid = row[\"cord_uid\"]\n",
    "        embedding = {}\n",
    "        file = path + cord_uid + \".txt\"\n",
    "        with open(file, \"rb\") as fp:   # Unpickling\n",
    "            embedding = pickle.load(fp)\n",
    "\n",
    "        #models = [bert_model, scibert_model, covid_model]\n",
    "        model_name = ['bert-base-uncased', \"allenai/scibert_scivocab_uncased\", \"gsarti/covidbert-nli\"]\n",
    "        #tokenizers = [bert_tokenizer ,scibert_tokenizer, covid_tokenizer]\n",
    "        #fields = [\"title\", \"abstract\"]\n",
    "        \n",
    "        #embedding[\"models\"][model_name[i]] = {}\n",
    "        field = row[field_name]\n",
    "\n",
    "        res = isinstance(field, str) \n",
    "        if (not res) or isNaN(field):\n",
    "            field = row[\"title\"]\n",
    "\n",
    "\n",
    "        embedd = tokenize(bert_model, bert_tokenizer, field)\n",
    "        embedding[\"models\"][model_name[model_id]][field_name] = embedd\n",
    "   \n",
    "\n",
    "        ## Timing the loops\n",
    "        if (idx % 1000) == 0:\n",
    "            print(idx)\n",
    "            print(\"Time (1000 iterations):\", round(time.time() - m,1))\n",
    "            m = time.time()\n",
    "\n",
    "        with open(file, \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(embedding, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Until now we have talked about term matching methods for information retrieval. While these methods are good they do have some issues. One issue is when the user is unfamiliar with what terms are used in relevant documents. A user might search for what percentage of people infected by covid die?, while the more 'correct' way might be What is the Covid case fatality rate. These two sentences have more or less the same semantic meaning, but can lead to different results when using term matching. What if we could try to extract the semantic meaning of a sentence, and match it with the query? For this task we will introduce sentence embedding models.\"\n",
    "string2 = \"Before looking more into sentence embedding models we will look at a simpler task; word embedding. The point of this task is to map a word into a vector. This vector should represents the meaning of the word. When the meaning of a word is represented as a vector it can be easily interpreted by a computer. One can understand the main idea of word embedding by a simple equation \"\n",
    "string3 = \"During the global covid-19 pandemic it has been important for researchers, politicians and others to be able to access the newest and most relevant research relating to COVID-19. The Semantic Scholar team at the Allen Institute for AI has partnered with leading research groups and released a dataset called CORD-19 \\cite{Wang2020CORD19TC}. The dataset aims to keep a corpus of COVID-19 research articles so researchers can apply recent advances in natural language processing to aid in the global effort against the pandemic. The CORD-19 dataset contains research articles relating to COVID-19. This includes articles on similar viruses like SARS and MERS, pandemics and other relevant topics. The dataset is updated daily. \"\n",
    "tokenize(covid_model, covid_tokenizer, string+string2+string3+string+string2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scibert_tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "scibert_model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_tokenizer = AutoTokenizer.from_pretrained(\"gsarti/covidbert-nli\")\n",
    "covid_model = AutoModel.from_pretrained(\"gsarti/covidbert-nli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\\"\n",
    "docs = pd.read_csv(path + \"crod_19_only_rel.csv\")\n",
    "docs.shape\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert embeddings\n",
    "\n",
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/User/Documents/NTNU/NLP/CORD-19/Embeddings/BERT/786/title/\"\n",
    "for idx, row in docs.iterrows():\n",
    "    BERT_embeddings_title = {}\n",
    "    sha = row[\"sha\"]\n",
    "    cord_uid = row[\"cord_uid\"]\n",
    "    BERT_embeddings_title[row[\"sha\"]] = tokenize(bert_model, bert_tokenizer, row[\"title\"])\n",
    "    if (idx % 10000) == 0:\n",
    "        print(idx)\n",
    "    file = path + cord_uid + \".txt\"\n",
    "    with open(file, \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(BERT_embeddings_title, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\embeddings\\\\\"\n",
    "\n",
    "with open(path + \"BERT_embedding_title.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(BERT_embeddings_title, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_embeddings_abstract = {}\n",
    "for idx, row in docs.iterrows():\n",
    "    BERT_embeddings_abstract[row[\"sha\"]] = tokenize(bert_model, bert_tokenizer, row[\"abstract\"])\n",
    "    if (idx % 10000) == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\embeddings\\\\\"\n",
    "\n",
    "with open(path + \"BERT_embedding_abstract.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(BERT_embeddings_abstract, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciBert embeddings\n",
    "\n",
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SciBERT_embeddings_title = {}\n",
    "for idx, row in docs.iterrows():\n",
    "    SciBERT_embeddings_title[row[\"sha\"]] = tokenize(scibert_model, scibert_tokenizer, row[\"title\"])\n",
    "    if (idx % 10000) == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\embeddings\\\\\"\n",
    "\n",
    "with open(path + \"SciBERT_embedding_title.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(SciBERT_embeddings_title, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SciBERT_embeddings_abstract = {}\n",
    "for idx, row in docs.iterrows():\n",
    "    SciBERT_embeddings_abstract[row[\"sha\"]] = tokenize(scibert_model, scibert_tokenizer, row[\"abstract\"])\n",
    "    if (idx % 10000) == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\embeddings\\\\\"\n",
    "\n",
    "with open(path + \"SciBERT_embedding_abstract.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(SciBERT_embeddings_abstract, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CordBert embeddings\n",
    "\n",
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CordBERT_embeddings_title = {}\n",
    "for idx, row in docs.iterrows():\n",
    "    CordBERT_embeddings_title[row[\"sha\"]] = tokenize(covid_model, covid_tokenizer, row[\"title\"])\n",
    "    if (idx % 10000) == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\embeddings\\\\\"\n",
    "\n",
    "with open(path + \"CordBERT_embedding_title.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(CordBERT_embeddings_title, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CordBERT_embeddings_abstract = {}\n",
    "for idx, row in docs.iterrows():\n",
    "    CordBERT_embeddings_abstract[row[\"sha\"]] = tokenize(covid_model, covid_tokenizer, row[\"abstract\"])\n",
    "    if (idx % 10000) == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\User\\\\OneDrive - NTNU\\\\NTNU\\\\Prosjekt oppgave NLP\\\\dataset\\\\CORD-19\\\\embeddings\\\\\"\n",
    "\n",
    "with open(path + \"CordBERT_embedding_abstract.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(CordBERT_embeddings_abstract, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding dim=786"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(num):\n",
    "    return num != num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/User/Documents/NTNU/NLP/CORD-19/Embeddings/786/\"\n",
    "n_models = 3\n",
    "n_fields = 2\n",
    "dim = 786\n",
    "\n",
    "start = time.time()\n",
    "m = start\n",
    "\n",
    "for idx, row in docs.iterrows():\n",
    "    embedding = {}\n",
    "    if (idx>21999):\n",
    "        sha = row[\"sha\"]\n",
    "        cord_uid = row[\"cord_uid\"]\n",
    "        embedding[\"sha\"] = sha\n",
    "        embedding[\"cord_uid\"] = cord_uid\n",
    "        embedding[\"dim\"] = dim\n",
    "        embedding[\"models\"] = {}\n",
    "\n",
    "        models = [bert_model, scibert_model, covid_model]\n",
    "        model_name = ['bert-base-uncased', \"allenai/scibert_scivocab_uncased\", \"gsarti/covidbert-nli\"]\n",
    "        tokenizers = [bert_tokenizer ,scibert_tokenizer, covid_tokenizer]\n",
    "        fields = [\"title\", \"abstract\"]\n",
    "\n",
    "        for i in range(n_models):\n",
    "            embedding[\"models\"][model_name[i]] = {}\n",
    "            for j in range(n_fields):\n",
    "                field = row[fields[j]]\n",
    "\n",
    "                res = isinstance(fields[j], str) \n",
    "                if (not res) or isNaN(field):\n",
    "                    field = row[\"title\"]\n",
    "              \n",
    "                \n",
    "                embedd = tokenize(models[i], tokenizers[i], field)\n",
    "                embedding[\"models\"][model_name[i]][fields[j]] = embedd\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        ## Timing the loops\n",
    "        if (idx % 1000) == 0:\n",
    "            print(idx)\n",
    "            print(\"Time (1000 iterations):\", round(time.time() - m,1))\n",
    "            m = time.time()\n",
    "\n",
    "        file = path + cord_uid + \".txt\"\n",
    "        with open(file, \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(embedding, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{\n",
    "sha: sha,\n",
    "cord_uid: cord_uid,\n",
    "dim: dim\n",
    "models: {model_name1: {title: [...],\n",
    "                       abstract: [...]},\n",
    "         model_name2: {title: [...],\n",
    "                       abstract: [...]},\n",
    "        ...                   \n",
    "}\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
